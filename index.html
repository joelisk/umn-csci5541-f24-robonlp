<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./docs/bulma.min.css" />

  <link rel="stylesheet" href="./docs/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./docs/css2" rel="stylesheet">
  <link href="./docs/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Task Sequencing in Robotics using Large Language Models</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">RoboNLP</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img height="325" width="325" src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/felix.jpg" alt="portrait">
            
            
          </div>
          <p>
                        
              Felix Su
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img height="325" width="325" src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/he.jpg" alt="portrait">
            
          </div>
          <p>
            
            Jiawei He
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img height="325" width="325" src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/fran.jpg" alt="portrait">            
            
          </div>
          <p>
              Franklin Xavier
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
                        
              <img height="325" width="325" src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/joeliskportrait.jpg" alt="portrait">
            
          </div>
          <p>
            Joseph Lisk
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Despite advancements in robotic capabilities, interaction with robotic manipulators remains constrained by the need for intricate programming and a lack of intuitive, non-manual control interfaces. Hence, we presents a framework for intuitive teleoperation of robotic manipulators through natural language commands, enabled by advancements in large language models (LLMs). We integrate an LLM-based action planner with a trajectory planner to interpret human speech and generate executable, sequential action plans for a simulated Kinova robotic arm. By evaluating eight LLMs with two prompting strategies on multiple tasks from the RLBench benchmark, we demonstrate the feasibility of translating natural language instructions into coherent, low-level manipulator actions without intricate manual programming. The system’s performance in a Gazebo-based simulation environment provides evidence that LLM-driven action sequencing can serve as a viable, accessible, and scalable approach to human-robot interaction, potentially broadening the utility of robotic assistance in complex, real-world scenarios.</p>

<hr>

<h2 id="teaser">Pipeline Figure</h2>

<p> This whole pipeline of our proposed framework.</p>

<p class="sys-img"><img src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/CSCI5541_pipeline_final.png" alt="imgname"></p>


<h3 id="the-timeline-and-the-highlights">Some demo video for the framework completing tasks from <a href="https://github.com/stepjam/RLBench">RLBench benchmark</a></h3>

<p>Picking:</p>

<video width="640" height="360" controls>
  <source src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/pick_up.mp4" type="video/mp4">
</video>

<p>Building a pyramid:</p>

<video width="640" height="360" controls>
  <source src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/pyramid.mp4" type="video/mp4">
</video>

<p>Place:</p>

<video width="640" height="360" controls>
  <source src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/place.mp4" type="video/mp4">
</video>

<p>Stacking:</p>

<video width="640" height="360" controls>
  <source src="https://joelisk.github.io/umn-csci5541-f24-robonlp/docs/assets/stack.mp4" type="video/mp4">
</video>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
  We propose a framework that integrates an LLM with a traditional trajectory planner to enable operators to command a robotic manipulator using natural human speech.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
  LLMs are being used to actuate robotic arms, but they are often limited by the complexity of the task and have limited analysis of how model parameter size and different prompting techniques affect the performance of this kind of pipeline.
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
  This work might contribute to the integration of the Human-Robot Interaction (HRI) field. We provide a potentially workable solution allowing non-expert operators to intuitively control the manipulator. This framework can be deployed on common workspace for humans and manipulators, so people from different backgrounds can all command the manipulator to do some tasks automatically to improve efficiency.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
  We developed a framework where a large language model (LLM) transforms spoken English instructions—transcribed by Faster Whisper—into sequences of predefined robotic actions. Instead of having the LLM generate code or trajectories directly, we restrict it to choosing from modular commands like “pick_up,” “go_to,” and “drop_to.” These high-level actions are then executed by a Kinova Gen3 arm in a Gazebo simulation, guided by established motion planning tools like MoveIt. This setup allows the LLM to focus on natural language comprehension and logical planning while the proven robotic trajectory commander like MoveIt handles the complexity of execution. The approach is relatively new in that it integrates LLM-driven reasoning with fixed, reliable action “building blocks” and continuously updated environmental information via ROS, ultimately making the system more robust, flexible, and practical for real-world applications.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
  In the initial idea of this framework, we want to integrate the LLMs and Variational autoencoder (VAE) to directly generate trajectories for the manipulator to execute. However, we found out it is time-consuming to collect data to train the VAE, and the LLMs are not reliable when they are asked to generate continuous trajectories, and it is hard to let the model understand all the spatial information. Hence, we decided to run the current approach at a very early stage since this problem is predictable. The other problem we encountered was we initially wanted to make the responses from the LLMs in JSON form in order to unify the output format. However, it actually makes it hard to write a Python script to read the output, so we decided to make the response in Python list format.
</p>

<hr>
    
<!-- <h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./docs/results.png">
</div>
<br><br>

<hr> -->

<h2 id="results">Results</h2>
<p>
<b>Initial qualitative results of action scheme</b>
</p>
<p>
For a basic assessment of LLMs with the JSON action scheme, we used Claude AI to generate a JSON where it was taked to make a sandwich as a robot arm. 
While the Claude AI came up with a logical action sequence, its final sandwich contained ingredients that aren't commonly used together.
</p>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./docs/assets/whisper.png">
</div>
<br> 
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./docs/assets/kinovasetup.png">
</div>
<br>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./docs/assets/systemprompt.png">
</div>
<br>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./docs/assets/userprompt.png">
</div>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./docs/assets/partialsuccess.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclusion and Future Work</h2>
<p>
  <!-- How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research? -->
  <b>Reproducibility</b>
  To reproduce our results, we've posted our code to our GitHub repository (see code link above)
  
  <b>Dataset</b>
  Using the RLBench dataset with LLMs opens the door to research into testing robotic systems on tasks with greater complexity. 
  Future research could delve into designing more complex tasks and integrating them with the RLBench framework and LLMs.

  <b>Ethical Concerns</b>
  Integrating human speech and LLMs into robot commands introduces the possibility for harmful errors, biases, and potentially malicious actions.
  One potential way to mitigate these concerns is with improved error-handling and developing policy-based filters.

  <b>Limitations</b>
  There appeared to be a limit to the amount of environment information that could be placed into the system prompt without
  causing the smaller LLMs to use object locations from the wrong task. Further research could study the limitations of 
  system prompts, which may be a function of model parameter size, for robot tasks.
</p>


<hr>


  </div>
  


</body></html>



grid of llms, prompting
avg steps before failing,
time to success